<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-02-16T02:11:36+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Lawrence Borst Blog</title><entry><title type="html">A formula for sums of powers</title><link href="http://localhost:4000/sums-of-powers" rel="alternate" type="text/html" title="A formula for sums of powers" /><published>2021-02-11T00:00:00+01:00</published><updated>2021-02-11T00:00:00+01:00</updated><id>http://localhost:4000/A%20Formula%20for%20Sums%20of%20Powers</id><content type="html" xml:base="http://localhost:4000/sums-of-powers">&lt;p&gt;In this post we will generalize a “geometric” approach to solving the problem of finding a closed form to&lt;/p&gt;

\[\sigma_{k}(n)=\sum_{i=0}^{n}i^k\]

&lt;p&gt;The formula I’ll derive is, as far as I’m aware after some . With keywords like “sums of powers, binomial coefficients, combinations”, I haven’t found anything similar except in the following paper by A.F. Beardon called “Sums of Powers of Integers”. I by no means suspect my formula is in any way new. In any case, I hope that the intuition offered here will be vitalizing to the reader.&lt;/p&gt;

&lt;p&gt;The problem, which to the best of my knowledge has no singular name, is a well-explored one. Many will have seen the following formula:&lt;/p&gt;

\[\sigma_{k}(n)=\frac{1}{k+1}\sum_{j=0}^{k}\begin{pmatrix}
k+1\\ 
j
\end{pmatrix}
B_{j}(n+1)^{k+1-j}\]

&lt;p&gt;There are similar formulae involving the Bernoulli numbers $B_{j}$, which were defined specifically for this problem. They can be defined recursively as:&lt;/p&gt;

\[B_{0}=1\]

\[\sum_{j=0}^{m}\begin{pmatrix}
m+1 \\
j
\end{pmatrix}
B_{j}=0\]

&lt;p&gt;We will take a different approach.
&lt;!--more--&gt;&lt;/p&gt;

&lt;h2&gt;The Geometric Approach for $\sigma_{1}(n)$ through $\sigma_{3}(n)$&lt;/h2&gt;

&lt;p&gt;We will return to middle school and think of numbers as blocks. In this case $\sigma_{1}(n)$ counts the number of blocks in a triangular stack of blocks. There’s a famous anecdote about young Carl Friedrich Gauss in arithmetic class being challenged by the teacher to solve this very problem ($\sigma_{1}(100)$). No sooner after he was given this problem, he returned the answer: $5050$. This he derived by summing $50$ pairs of $101$ by appropriate grouping of the terms in the sum.&lt;/p&gt;

&lt;p&gt;Many use this as a testament to Gauss’ early genius, which it certainly is. I, however, take the central message to be: “work smart, not hard”.&lt;/p&gt;

&lt;p&gt;Back to the problem at hand, we see that a similar grouping can be performed for general $n$—albeit there are two cases (even and odd $n$). In this way we see that $\sigma_{1}(n)=\frac{1}{2}n(n+1)=\begin{pmatrix}
n+1\ 
2
\end{pmatrix}$.&lt;/p&gt;

&lt;p&gt;So how do we get $\sigma_{2}(n)$? Well, what does $\sigma_{2}(n)$ signify in terms of blocks? It is a sum of squares… So a sum of geometrical squares. Could we move $k$ up the ladder to $k=2$ by using information about $\sigma_{1}(n)$, where $k=1$?&lt;/p&gt;

&lt;p&gt;Well, if $\sigma_{1}(n)$ represents a right triangle of blocks, it’s not hard to see how $2\sigma_{1}(n)$ approximates the number of blocks in a rectangle of length $n$ and height $n+1$.&lt;/p&gt;

&lt;p&gt;Indeed, the reader will perhaps visualize $2\sigma_{1}(n)$ as two right triangles of blocks, with one rotated, to fill a rectangle of length $n$ and height $n+1$. This discussion lends itself better to visualizing $2\sigma_{1}(n)$ as a single square of length and height $n$ from two right triangles that overlap at the diagonal (so we overcount by $n$ blocks). Here’s why:&lt;/p&gt;

&lt;p&gt;$\sigma_{2}(n)$ is clearly the sum of squares of blocks. Each square is represented by $2\sigma_{1}(n)$, or $2$ right triangles of blocks, minus the overlap $n$ (a line of blocks). Therefore&lt;/p&gt;

\[\sigma_{2}(n)=\sum_{i=0}^{n}\sigma_{1}(i)-\sum_{i=0}^{n}i=\sum_{i=0}^{n}\sigma_{1}(i)-\sum_{i=0}^{n}\sigma_{0}(i)\]

&lt;p&gt;That looks terribly complicated! Certainly the last sum is just $\sigma_{1}(n)$, but how about the first? Well, actually&lt;/p&gt;

\[\sum_{i=0}^{n}\begin{pmatrix}
i+1\\ 
2
\end{pmatrix}
=
\begin{pmatrix}
n+2\\ 
3
\end{pmatrix}\]

&lt;p&gt;The reader should note that, in the above, the binomial coefficients on the left are very similar to those on the right, except on the right we’ve augmented top and bottom by $1$. One proof is simply this: the LHS represents a sum, and therefore the RHS is the cumulative value of that sum. Therefore to prove the equality above we require that $\begin{pmatrix}
n+2\ 
3
\end{pmatrix}
-
\begin{pmatrix}
n+2-1\ 
3
\end{pmatrix}
=
\begin{pmatrix}
n+1\ 
2
\end{pmatrix}$. This is easily checked to be true. We also require that $\begin{pmatrix}
0+2\ 
3
\end{pmatrix}
=
\begin{pmatrix}
0+1\ 
2
\end{pmatrix}$. The first condition is simply saying that the difference of RHS terms for consecutive $n$ should equal the last term in the LHS sum for choice of $n$. The second condition is saying that the LHS and RHS “start” at the same spot for $n=0$.&lt;/p&gt;

&lt;p&gt;This little trick can be used to prove the general case: for any positive $n$, $k$ (equation $1$):&lt;/p&gt;

\[\sum_{j=0}^{n}\begin{pmatrix}
j+k\\ 
1+k
\end{pmatrix}
=
\begin{pmatrix}
n+k+1\\ 
1+k+1
\end{pmatrix}\]

&lt;p&gt;Equation $1$ will be integral to our discussion.&lt;/p&gt;

&lt;p&gt;Returning to $\sigma_{2}(n)$. It is clear now that
\(\sigma_{2}(n)
=
2\begin{pmatrix}
n+2\\ 
3
\end{pmatrix}
-
\begin{pmatrix}
n+1\\ 
2
\end{pmatrix}\)&lt;/p&gt;

&lt;p&gt;To recapitulate, the first term represents the sum of right triangles made of blocks. The minus part takes away the overlap.&lt;/p&gt;

&lt;p&gt;Now, on to $\sigma_{3}(n)$. Can we do something similar? Let’s see, $\sigma_{3}(n)$ represents a sum of cubes. How can we make cubes from whatever $\sigma_{2}(n)$ represents? Well, let’s see what $\sigma_{2}(n)$ represents. It is a sum of squares; summing gives a pyramid, analoguous to the right triangle we’ve associated with $\sigma_{1}(n)$.&lt;/p&gt;

&lt;p&gt;So can we make cubes from pyramids? Certainly we can, and this picture shows how:&lt;/p&gt;

&lt;p&gt;PICTURE WILL BE ADDED&lt;/p&gt;

&lt;p&gt;But we should note that our pyramids are blocky. This causes overlap, and I’ll describe exactly the overlap:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We have three pyramids.&lt;/li&gt;
  &lt;li&gt;$3$ sides are overlapping (two pyramids overlap).&lt;/li&gt;
  &lt;li&gt;There is exactly $1$ block where all three pyramids overlap.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Great. Now, let’s translate this to $\sigma_{3}(n)$. Here are we taking sums of cubes. Each cube will consist of these pyramid, but we will have to do something analoguous to inclusion-exclusion. We will remove the overlap across the pyramids. But we will add back the block we accidentally remove. This gives equation (2):&lt;/p&gt;

\[\sigma_{3}(n)=3\sum_{i=0}^{n}\sigma_{2}(i)-3\sum_{i=0}^{n}\sigma_{1}(i)+\sum_{i=0}^{n}\sigma_{0}(i)\]

&lt;p&gt;The reader may recognize one piece of intuition: $\sum_{i=0}^{n}\sigma_{k}(i)$ represents a kind of $k+1$-dimensional simplex with a right angle in it.&lt;/p&gt;

&lt;p&gt;We shall use equation $(1)$ to sum each of these components. Something remarkable happens (because of equation $(1)$); we get to use our expressions for $\sigma_{k}(n)$, except we need only augment the terms in the expression by $1$! Therefore:&lt;/p&gt;

\[\sigma_{3}(n)=3\left ( 2\binom{n+3}{4}-\binom{n+2}{3} \right )-3\binom{n+2}{3}+\binom{n+1}{2}\]

&lt;p&gt;How do we quickly check this to be true? Armed with the common knowledge that the LHS is a polynomial of degree $3+1=4$, and so is the RHS (clearly), it is clear we only need to check $3$ values of $n$ (3 points). This would make both the LHS and RHS the unique interpolating polynomial for those $3$ points.&lt;/p&gt;

&lt;p&gt;Will this generalize to higher dimensions? Maybe. But how?&lt;/p&gt;

&lt;p&gt;I can’t speak exactly for what happens in higher dimensions, but squinting our eyes at equation (2), or other similar expressions we obtained for $\sigma_{1}(n)$, $\sigma_{2}(n)$, $\sigma_{3}(n)$ we see the familiar entries in Pascal’s triangle as coefficients. It seems, in general, that:&lt;/p&gt;

\[\sigma_{k}(n)=\sum_{i=0}^{k - 1} (-1)^{i + k + 1} \binom{k}{i} \sum_{j=0}^{n} \sigma_{i}(j)\]

&lt;p&gt;Remember that to evaluate $\sum_{j=1}^{n} \sigma_{k}(j)$ we simply augment the upper and lower part of the binomial coefficients appearing in the expression by $1$, making $\sigma_{k+1}(n)$ not all that difficult to produce inductively.&lt;/p&gt;

&lt;p&gt;For instance:&lt;/p&gt;

\[\sigma_{4}(n)=4 \left ( 3 \left ( 2 \binom{n+4}{5} - \binom{n+3}{4} \right ) - 3 \binom{n+3}{4} + 2 \binom{n+2}{3} \right ) - 6 \left ( 2 \binom{n+3}{4} - \binom{n+2}{3} \right ) + 4 \binom{n+2}{3} - \binom{n+1}{2}\]

&lt;p&gt;With my limited experience I can only make a guess as to what’s happening. The binomial coefficients $(-1)^{i+n+1} \binom{k}{i}$ appear as part of a inclusion-exclusion-like expression, and indeed we have seen that in our block-based geometric approach we are kind of overshooting and undershooting the count of blocks. Exactly what is the nature of this overshooting and undershooting? It seems that our $n$-dimensional cube can be made up of $n$ $n$-simplices with a right corner. It’s also clear that overshooting or undershooting can only occur at the boundary of the simplex. So let’s say we want to compute the number of blocks in an $n$-dimensional cube based on these simplices. As a first approximation, it is simply $n$ times the number of simplices, which we recognize as $\sigma_{n}{N}$, where $N$ represents the “length” or so of the simplex. But we overcount where the boundaries intersect. The boundary of the right-angled $n$-dimensional simplex is made up of $n-1$-dimensional right-angled simplexes (roughly… Actually, the one such simplex we are looking at is not right-angled, but is oriented in such a way, “diagonally”, so that precisely the right number of little blocks will fit surround it). Let’s say, for simplicity, this boundary consists of $n-1$-dimensional “edges”. Then our overcount will be the number of blocks over the $n-1$-dimensional “edges” of our $n$-simplices. As a first approximation, this equals the number of blocks on a given such “edge” times the number of such “edges”. Guessing that all the $n$-simplices touch each other, it follows that all these “edges” occur exactly where $2$ intersect $n$-simplices intersect. Since there are $n$ $n$-simplices, the number of $n-1$-dimensional “edges” is $\binom{n}{n-2}$. Now we undercount for the boundary of each “edge”, and so we go down another dimension, looking at $n-2$-dimensional “edges”. In this case $3$ $n$-simplices intersect to form such an “edge”, and the number of them is $\binom{n-3}{3}$. You get the gist. This is the intuition I have.&lt;/p&gt;

&lt;p&gt;But if you want proof, read on.&lt;/p&gt;

&lt;p&gt;Theorem:&lt;/p&gt;

\[\sigma_{k}(n)=\sum_{i=0}^{k - 1} (-1)^{i + k + 1} \binom{k}{i} \sum_{j=0}^{n} \sigma_{i}(j)\]

&lt;p&gt;Proof: We use strong induction on $k$. It is easily verified that $k=0$ works. Firstly, let us use the inductive hypothesis to expand $\sigma_{i}(j)$&lt;/p&gt;

\[\sigma_{k}(n)=\sum_{i=0}^{k-1}\left ( -1 \right )^{i+k+1}\binom{k}{i}\sum_{j=1}^{n}\sum_{l=1}^{j}l^{j}\]

&lt;p&gt;This is a finite sum, so we can happily rearrange it:&lt;/p&gt;

\[\sigma_{k}(n)=\sum_{j=1}^{n}\sum_{l=1}^{j}\sum_{i=0}^{k-1}\left ( -1 \right )^{i+k+1}\binom{k}{i}l^{j}\]

&lt;p&gt;Our attention should be on the innermost sum. Why? Because all these binomial coefficients should lead us to the binomial theorem. Indeed, it is easily verified that:&lt;/p&gt;

\[\sum_{i=0}^{k-1}\left ( -1 \right )^{i+k+1}\binom{k}{i}l^{j}=\sum_{i=0}^{k}\left ( -1 \right )^{i+k+1}\binom{k}{i}l^{j}+l^{k}=\left ( 1 - l \right )^{k}\left ( -1 \right )^{k+1}+l^{k}\]

&lt;p&gt;Therefore:&lt;/p&gt;

\[\sigma_{k}(n)=\sum_{j=1}^{n}\sum_{l=1}^{j}\left ( \left ( 1 - l \right )^{k}\left ( -1 \right )^{k+1}+l^{k} \right )\]

&lt;p&gt;We would clearly be done if we could show that this inner sum equals $i^{k}$. This is indeed the case, and I leave it to the reader to check that all the terms in the inner sum, except for $j^{k}$ neatly cancel out. This leaves:&lt;/p&gt;

\[\sigma_{k}(n)=\sum_{j=1}^{n}j^k\]

&lt;p&gt;Which is what we aimed to show.&lt;/p&gt;

&lt;p&gt;I would like to add that our formula, representing $\sigma_{k}(n)$ in terms of binomial coefficients, allows us to easily take not only sums of $\sigma_{k}(n)$ over $n$, but also sums over these sums ad infinitum. That’s certainly an interesting perk.&lt;/p&gt;

&lt;p&gt;What remains, of course, is explicitly knowing the integers/coefficients in front of our binomial coefficients without inductively invoking the theorem. I sadly couldn’t find a simple closed form for these integers. Perhaps the reader can.&lt;/p&gt;</content><author><name>Lawrence Borst</name></author><category term="mathematics" /><category term="sum" /><category term="sigma" /><category term="summation" /><category term="powers" /><category term="choose" /><category term="combinations" /><category term="combinatorics" /><category term="number theory" /><summary type="html">In this post we will generalize a “geometric” approach to solving the problem of finding a closed form to \[\sigma_{k}(n)=\sum_{i=0}^{n}i^k\] The formula I’ll derive is, as far as I’m aware after some . With keywords like “sums of powers, binomial coefficients, combinations”, I haven’t found anything similar except in the following paper by A.F. Beardon called “Sums of Powers of Integers”. I by no means suspect my formula is in any way new. In any case, I hope that the intuition offered here will be vitalizing to the reader. The problem, which to the best of my knowledge has no singular name, is a well-explored one. Many will have seen the following formula: \[\sigma_{k}(n)=\frac{1}{k+1}\sum_{j=0}^{k}\begin{pmatrix} k+1\\ j \end{pmatrix} B_{j}(n+1)^{k+1-j}\] There are similar formulae involving the Bernoulli numbers $B_{j}$, which were defined specifically for this problem. They can be defined recursively as: \[B_{0}=1\] \[\sum_{j=0}^{m}\begin{pmatrix} m+1 \\ j \end{pmatrix} B_{j}=0\] We will take a different approach.</summary></entry><entry><title type="html">Generalizing graph theory</title><link href="http://localhost:4000/complexes" rel="alternate" type="text/html" title="Generalizing graph theory" /><published>2020-12-01T00:00:00+01:00</published><updated>2020-12-01T00:00:00+01:00</updated><id>http://localhost:4000/Generalizing%20Graph%20Theory</id><content type="html" xml:base="http://localhost:4000/complexes">&lt;p&gt;Here we will look at a generalization of graph theory. All visualizations were made in Blender3D.&lt;/p&gt;

&lt;p&gt;There are a few ways to generalize graph theory. One way is with the “hypergraph”, which in essence is a graph where edges can connect to more than $2$ vertices. This is easy to visualize if we imagine edges as being able to “branch”. A bit of imagination can lead us to other generalizations, and here I present one I came up with that motivates the rich theory of CW complexes. In particular, it is a kind of “intermediary” generalization between that of so-called simplicial and CW complexes.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2&gt;Generalizing Finite, Connected Planar Graphs and Euler's Theorem&lt;/h2&gt;
&lt;p&gt;Let us try to generalize connected simple planar graphs, because they are quite “nice”. Firstly, they are graphs, but they are also fundamentally geometric, as planar graphs are those graphs that can be embedded in the plane. We will extend two theorems: firstly, connected simple planar graphs have a property (Euler’s Theorem) connecting $v$, $e$, $f$, where these are the number of vertices, edges and faces in the graph respectively.&lt;/p&gt;

&lt;p&gt;Euler’s Theorem: If a finite, connected planar graph is drawn in the plane without edge intersections, then&lt;/p&gt;

\[v-e+f=2\]

&lt;p&gt;Secondly, planar graphs can be embedded onto the plane without any two edges intersecting. We assume the reader has a “nice” mental representation of edges, without having to go into strict formalisms. The reader may note how neither vertices, edges nor faces intersect other vertices, edges or faces respectively. In fact(!), observe that—if we identify vertices as points, edges as lines and faces as bounded open sets filling in the “space” between edges (polygons)—neither points, edges or faces intersect other points, edges or faces.&lt;/p&gt;

&lt;p&gt;Euler’s theorem is a little misleading as it stands now because I haven’t quite defined what constitutes a “face”. In particular, for graphs that can be embedded into a bounded subset of the plane, we still take the “outside” of the graph to be counted as a face. We shall see, rather surprisingly, that in the generalization here it is maybe more elegant to say that the “outside” does not constitute a face and that $v-e+f=1$, although this is far from the norm for deeper reasons. You can easily verify Euler’s theorem for the triangle below:&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../assets/css/img/Graph1.png&quot; alt=&quot;Triangle&quot; /&gt;&lt;/center&gt;

&lt;p&gt;In fact, there is a lot to be tightly formulated. But this is not necessary, because I only want to illustrate the ideas involved in generalizing planar graphs.&lt;/p&gt;

&lt;p&gt;The first thing to note is that connected simple planar graphs are, well, planar. That is, they are clearly embedded into the 2D plane, i.e. 2D euclidean space. We might expect a generalization to be 3-dimensional. It is helpful to think of the graphs as “living” in a certain space, not allowed to extend outside it. The obvious generalization is what computer scientists call a polygon mesh, which intuitively can be viewed as a volume whose boundary is a set of poylgons (though this does not rule out pathological cases). Examples are, of course, polyhedra, but also complicated volumes that have been, as they say, triangulated. Indeed we shall go with polygonal meshes.&lt;/p&gt;

&lt;p&gt;In computer graphics, a polygonal mesh is basically a set of vertices, edges and faces. A polygonal mesh may have intersecting vertices, edges and faces, and may be, for the interest of the reader, be called “non-manifold” by 3D modeling artists.&lt;/p&gt;

&lt;p&gt;That aside, we notice how a polygonal mesh “glues” to another polygonal mesh; it is quite clear two polygonal meshes can be glued by sharing a face. Let’s go with that idea, because in the planar graphs we have discussed a face can be glued to a face by an edge, and an edge can be glued to an edge by a vertex. Furthermore, we don’t necessarily disallow gluing faces together by vertices; for this reason we can glue polygonal meshes by vertices, or edges as well. This is all very elegant.&lt;/p&gt;

&lt;p&gt;For the sake of brevity, let us call the generalization of a “face” a “room”. The following shape has $1$ room, $4$ faces, $6$ edges and $4$ vertices.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../assets/css/img/Graph2.png&quot; alt=&quot;Tetrahedron&quot; /&gt;&lt;/center&gt;

&lt;p&gt;The following more complicated shape has $2$ rooms, $10$ faces, $15$ edges and $8$ vertices. Notice how, when two rooms are glued together, they share &lt;i&gt;one&lt;/i&gt; face, not two faces. If they shared two faces, then these faces would intersect, which is forbidden. In particular, we forbid the interior of the faces to intersect. In the general case, we disallow the interior of any two rooms, faces, edges or vertices to intersect.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../assets/css/img/Graph3.png&quot; alt=&quot;Cube with inner face&quot; /&gt;&lt;/center&gt;

&lt;p&gt;Now, let us try to generalize Euler’s formula. We will, for simplicity, disregard the face “outside” the shape, so that for simply connected planar graphs $V-E+F=1$. If a generalization for the planar case exist to higher dimensions, then we might expect the existence also of a “reduction” to lower dimensions. In the one dimensional space, we have, for simplicity, a graph that we may draw on a “line”. The rules are: (1) we may draw a vertex or an edge and (2) edges and vertices may not intersect other edges or vertices. Furthermore, we shall not take the “outside” part as an edge. The reader can convince him/herself that such graphs satisfy $V-E=1$. If we had taken the “outside” as an edge, we would have $V-E=0$. Now let’s go zero-dimensional. We have a euclidean space that consists of a single point. This point can be the vertex, but no edges can be added (because they are one-dimensional) and no additional vertices (because (1) vertices would intersect and (2) there would be no edges connecting them). Clearly Euler’s theorem is then $V=1$.&lt;/p&gt;

&lt;p&gt;So we have seen that Euler’s theorem for $n=0$, $1$, $2$ is $V=1$, $V-E=1$ and $V-E+F=1$. There is a clear pattern, and let us conjecture that in 3D $V-E+F-R=1$ where $R$ is the number of “rooms”.&lt;/p&gt;

&lt;p&gt;Looking at the two 3D shapes given, we indeed find that $V-E+F-R=1$ in both cases(!). It seems like we have found a valid generalization of Euler’s theorem, and this is indeed the case.&lt;/p&gt;

&lt;h1 id=&quot;cw-complexes&quot;&gt;CW-Complexes&lt;/h1&gt;
&lt;p&gt;We will see that the above discussion ties in very naturally with an accepted generalization of graphs, namely CW-complexes. These are more abstract, topological objects that generalize the hierarchy of vertices, edges, faces and so on, with rules prescribing how they are combined. The complex itself represents the graph. The definition is particularly simple in the case of finite graphs. CW-complexes are made from $n$-cells, so let’s first define those. We work in a topological space $X$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt;: An $n$-cell is a space homeomorphic to the open $n$-disk $int(D^{n})$. A cell is a space which is an $n$-cell for some $n \geq 0$.&lt;/p&gt;

&lt;p&gt;Here we define $D^{n} = \left \{ x \in \mathbb{R}: \left | x \right | \leq 1 \right \}$. The $n$-cell aims to generalize vertices, edges, faces and so on. It’s quite clear how this works. Note that we can talk about &lt;i&gt;the&lt;/i&gt; dimension of an $n$-cell, because $int(D^{n})$ and $int(D^{m})$ are homeomorphic iff $n=m$. To understand the connection of $n$-cells with $n-1$-cells we need to define the boundary of the $n$-dimensional disk. We define it as $S^{n-1} = \left \{ x \in \mathbb{R}: \left | x \right | = 1 \right \}$. We also need the convention $D^{0}={0}$. This is of course homeomorphic to a point in space; namely a vertex in our analogy.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt;: A cell-decomposition of a space $X$ is a family $\mathcal{E} = \left \{ e_{\alpha} : \alpha \in I \right \}$ (where $I$ is an index) of subspaces of $X$ such that each $e_{\alpha}$ is a cell and&lt;/p&gt;

\[X = \coprod_{\alpha \in I} e_{\alpha}\]

&lt;p&gt;(&lt;i&gt;disjoint&lt;/i&gt; union). The $n$-skeleton of $X$ is the subspace&lt;/p&gt;

\[X^{n} = \coprod_{\alpha \in I : dim(e_{\alpha}) \leq n} e_{\alpha}\]

&lt;p&gt;Think of the $n$-skeleton as housing all the $n$-cells, $n-1$ cells, and so on. For the 3D meshes from earlier, $X^{2}$ would be a wireframe including edges and vertices, but not faces.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt;: A pair $\left ( X, \mathcal{E} \right )$ consisting of a Hausdorff space $X$ and a cell-decomposition $\mathcal{E}$ of $X$ is called a CW-complex if the following $3$ axioms hold:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;- (‘Characteristic Maps’) For each $n$-cell $e \in \mathcal{E}$ there is a map $\Phi_{e} : D^{n} \rightarrow X$ restricting to a homeomorphism $\Phi_{e} |_{int(D^{n})} \rightarrow e$ and taking $S^{n-1}$ into $X^{n-1}$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;- (‘Closure Finiteness’) For any cell $e \in \mathcal{E}$ the closure $\overline{e}$ intersects only a finite number of other cells in $\mathcal{E}$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;- (‘Weak Topology’) A subset $A \subseteq X$ is closed iff $A \cap \overline(e)$ is closed in $X$ for each $e \in \mathcal{E}$.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Intuitively the first axiom tells us two things: (1) that each cell can be built from the $n$-disc, should we allow continuously deforming it into $X$, and (2) that the boundary of such a cell is restricted to $X^{n-1}$. The first part intuitively captures the natural progression from points, to curves, to surfaces, to volumes that we have seen in graphs. The second part, based on our previous discussion, says that faces have boundaries that contain vertices and edges, but NOT “rooms”. In graph theory we would naturally not define the boundary of an edge to be a face; it makes much more sense for it to be a vertex—to go down a dimension. This is captured here. Furthermore, this tacitly embodies the idea that graphs are made from “gluing” together different components.&lt;/p&gt;

&lt;p&gt;The second axiom is more subtle, and rules out pathological cases (for instance edges made up of infinitely many vertices). The same can be said of the third axiom, and in fact they are both superfluous if $\mathcal{E}$ is finite i.e. there are a finite number of cells. We are only concerned with finite complexes.&lt;/p&gt;

&lt;p&gt;The generalization we sought to find was that of the Euler characteristic for finitely many cells. We define the Euler characteristic of a finite CW complex as:&lt;/p&gt;

\[\chi = k_{0} - k_{1} + k_{2}...\]

&lt;p&gt;Where $k_{n}$ are the number of $n$-cells. In the 3D case, the Euler characteristic is analoguous to the number of “holes” in a complex. In our discussion, as we have seen, we would correctly expect the Euler characteristic of $S^{n}$ to equal $1 + (-1)^{n}$ (here we also take the “outside” of the sphere as an $n$-cell, because it is included in the space $X$).&lt;/p&gt;

&lt;p&gt;To end with, there is another generalization of the graph slightly weaker the notion of CW-complexes; namely that of the simplicial complex. Here graphs are made up of mathematical simplexes, where an $n$-dimensional simplex consists of $n+1$ points. In the mathematical literature, the simplex of the highest dimension is called a chamber. Words like “apartment” and “building” also come up in this theory, which I find very neat. In three dimensions, we can imaginatively place ourselves inside a large cube (atrium) with twelve doors to different corridors (edges), and in the corners a total of six more doors to smaller rooms (vertices). Going into a corridor, we can see it stretch left and right between two opposite doors, leading again to the aforementioned rooms. This is, of course, a cube! Neat, huh?&lt;/p&gt;

&lt;h1&gt;References&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Soren Hansen. &lt;i&gt;CW Complexes&lt;/i&gt;. (Lecture Notes, Kansas State University)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Gowers, Timothy, June Barrow-Green, and Imre Leader. 2008. &lt;i&gt;The Princeton Companion to Mathematics&lt;/i&gt;. Princeton, N.J.: Princeton University Press.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Lawrence Borst</name></author><category term="mathematics" /><category term="math" /><category term="graph" /><category term="theory" /><category term="CW" /><category term="complex" /><category term="complexes" /><summary type="html">Here we will look at a generalization of graph theory. All visualizations were made in Blender3D. There are a few ways to generalize graph theory. One way is with the “hypergraph”, which in essence is a graph where edges can connect to more than $2$ vertices. This is easy to visualize if we imagine edges as being able to “branch”. A bit of imagination can lead us to other generalizations, and here I present one I came up with that motivates the rich theory of CW complexes. In particular, it is a kind of “intermediary” generalization between that of so-called simplicial and CW complexes.</summary></entry><entry><title type="html">Quantum computation and search</title><link href="http://localhost:4000/quantum-computing-an-introduction" rel="alternate" type="text/html" title="Quantum computation and search" /><published>2020-12-01T00:00:00+01:00</published><updated>2020-12-01T00:00:00+01:00</updated><id>http://localhost:4000/Quantum%20Computation%20and%20Search</id><content type="html" xml:base="http://localhost:4000/quantum-computing-an-introduction">&lt;p&gt;During the past few months, I’ve been reading on quantum computation. For simplicity, I’ve listed all the resources in the bibliography for the ease of the reader. I strongly recommend to others who wish to learn about quantum computing to start with &lt;a style=&quot;color:blue;&quot; href=&quot;https://www.amazon.com/Quantum-Computing-Computer-Scientists-Yanofsky/dp/0521879965&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferer&quot;&gt;Quantum Computing for Computer Scientists&lt;/a&gt; and just skim through it. This is a slightly dumbed-down approach to quantum computing, but very practical nonetheless. Going through it will equip you with all the knowledge you’d need to make your own quantum computinng emulator, assuming you know a little programming. The next best thing is the quantum bible &lt;a style=&quot;color:blue;&quot; href=&quot;https://www.amazon.com/Quantum-Computation-Information-10th-Anniversary/dp/1107002176&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferer&quot;&gt;Quantum Computation and Quantum Information&lt;/a&gt;, which is more challenging but gives you a more grounded understanding of quantum computation, as well as some of the physics behind it. Besides those resources, I recommend anyone interest in quantum computation to play with &lt;a style=&quot;color: blue;&quot; href=&quot;https://qiskit.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferer&quot;&gt;Qiskit&lt;/a&gt;—a quantum emulator—and to read through its documentation. The stack exchange for quantum computation is also a useful resource.&lt;/p&gt;

&lt;p&gt;In this introductory article, I hope to lay out the groundworks for any beginner in the manner that these sources describe. I hope this will pique the interest of someone who may have heard of quantum computing, but has no clue what it really entails.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2&gt;The Quantum Circuit Model&lt;/h2&gt;

&lt;p&gt;We begin with some intuition as to what a quantum computer is. Like a classical computer, we expect that quantum computers have registers which store the quantum-equivalent of bits. These quantum registers store qubits. And like a classical computer, which are simulated by the tried-and-tested circuit model of computation, we expect quantum computers to be implemented in the same way.&lt;/p&gt;

&lt;p&gt;To get slightly more abstract, this fancy circuit model is really a model of computation equivalent to the Turing model. It is equivalent in the sense that any circuit can simulate an arbitrary Turing machine (a computer running on the rules the turing model prescribes to), and vice versa. The usefulness of the circuit model then follows from the usefulness of the turing model by the Church Turing Thesis.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Church Turing Thesis:&lt;/strong&gt; Every effectively calculable function is a computable function.&lt;/p&gt;

&lt;p&gt;According to Turing, “effectively calculable” refers to the “intuitive idea” of a function that can be calculated by any means. A computable function is one that can be computed on a turing machine-that is, if $f(x)$ is defined, then a turing machine given an input $x$ (encoded to fit the model, that is) will halt with $f(x)$ stored in memory; furthermore, if undefined, the turing machine will run forever or get stuck.&lt;/p&gt;

&lt;p&gt;All these details are not too crucial for a starter, except it is true that an important aim in quantum computation is to show that quantum computers can theoretically perform the same feats as classical computers. This can be effectively showed by showing that the quantum circuit model (the one used by a quantum computer) is equivalent to the classical circuit model. This is indeed the case. This means that classical computers can effectively simulate quantum computers, proving that our emulators can indeed work. It is interesting to note that quantum turing machines are a thing, but likely lost to the quantum circuit model in the same way that classical turing machines lost the classical model; namely for being less practical. The circuit models more realistically encapsulate computers we can build.&lt;/p&gt;

&lt;p&gt;As we know from a classical computer, once we have our bits in a register we can perform operations on them. In particular, we apply binary functions to our (generally 64-bit) registers. So a single register can be ascribed completely to a point in ${0, 1}^{64}$, and can represent $2^{64}$ “states” as it were. Mathematically, we would call ${0, 1}^{64}$ the state space. That’s a lot of states. A register, as it were, holds some finite amount of information. A quantum register also holds information. In theory, it holds an infinite amount of information, although in practice the information we can observe is the strictly finite $2^{64}$ states. The hidden information that we can play with, however, is much larger, and can be represented by a set of real numbers between $-1$ and $1$ in a space of a whopping $2^{(2^{63})}$ dimensions! Not only that, but unlike classical registers, the individual qubits can be entangled, thus storing information in the form of relationships between the qubits. In quantum computing, it is primarily this information which we manipulate. Not all applications can necessarily be implemented in a way that exploits this entanglement, because it still abides by certain rules. We therefore don’t expect a quantum computer to be some magic machine that speeds up all problems. But it has been shown to be asymptotically faster-sometimes exponentially so-on a subset of our problems of interest.&lt;/p&gt;

&lt;p&gt;According to Quantum Computation and Quantum Information (listed below), a quantum computer has the following components:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Classical resources:&lt;/strong&gt; a classical part. Because quantum computers cannot practically speed up all problems, and can even be ill-suited for some, it is useful to have a classical part. We allocate a process to the part which is best suited for it. For storing and reading information into and from the quantum computer, we would use a classical machine. Moreover, we might use a quantum computer only for certain subroutines in the case of more complex processes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A suitable state space:&lt;/strong&gt; as said, quantum computers will act on some number $n$ of qubits. The state space is a $2^{n}$-dimensional complex Hilbert space in which the state of the system lives. This state encodes all the qubits.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The ability to prepare states in the computational basis:&lt;/strong&gt; the computational basis is a basis in which each individual qubit starts out in the same basis, and the total system is represented by the tensor product of these states; we’ll so more closely what this entails.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ability to perform quantum gates:&lt;/strong&gt; because we fundamentally choose a quantum circuit model we expect to be able to use quantum gates on individual qubits, as well as sets of qubits. This is analoguous to being able to apply gates to bits, which are binary functions on their states. We’ll see that for qubits we’ll be using unitary operators instead. It is also mentioned that a universal set of gates should be implemented. These are gates whose representation as operators can be combined (multiplied) in a way that can arbitrarily approximate any unitary operator. In simple terms, these are gates that can pretty much implement any function we want on qubits.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ability to perform measurements in the computational basis:&lt;/strong&gt; we most certainly want to run algorithms on quantum computers and measure their output. This output would be meaningless if we do not agree on a specific encoding of the information, which we naturally choose to be the defined through the computational basis.&lt;/p&gt;

&lt;h2&gt;Quantum Circuits as Graphs&lt;/h2&gt;
&lt;p&gt;As a pedagogical tool, we’ll use graphs analoguous to quantum circuits to explore the concept of quantum gates, and how they differ from classical gates. The reader may recognise the use of Markov processes to explain quantum circuits.&lt;/p&gt;

&lt;p&gt;Let us imagine that instead of qubits, our circuits simply acts on the vertices of a graph. We suppose we only have one qubit, represented by a marble. As with qubits, we only have partial information about this marble. In our case, for any vertex $V$ of our circuit, there is an associated number representing the probability of finding the marble at $V$. Next, the edges of the graph, which are directed so have a well-defined “start” and an “end”, represent some edge-specific probability $p$ of any marbles in the “start” vertex $V_{s}$ to jump to the “end” vertex $V_{e}$. We expect that if $V_{s}$ holds the marble with probability $P$, then there is a chance $Pp$ that the marble will jump to $V_{e}$. But of course, this system is complicated, and the “virtual” marble from $V_{e}$ maybe travel back to $V_{s}$ if there is some way to reach it. Furthermore, the expected number of marbles that cross the edge will change with time, because the expected number of marbles held at any given vertex changes. That is, this system has “dynamics”. The state of the system evolves from time $t$ to $t+dt$ in some well-defined manner. We need a way to describe the dynamics. If we encode the state described by $V_{1},…,V_{n}$ as $\psi(0)=\left [ P_{0},…,P_{n-1} \right ]^{T}$, where $P_{i}$ is the aforementioned probability of finding the marble at $V_{i}$, then the state evolves as:&lt;/p&gt;

\[\psi(t+dt)=U\psi(t)\]

&lt;p&gt;Where $U$ is some linear function on $\mathbb{R}^{n}$. Now, since we are talking about probabilities, we cannot expect $U$ to tell us precisely what state $\psi$ will be in after some time $dt$, because every time we run the same simulation we would expect a slightly different outcome. Instead, $U$ will be used to encode the state we expect $\psi$ to enter; that is, the most likely state after some time. In fact, $\psi(t)$ specifically encodes the probability of finding a marble at any given vertex, and at any time $t$.&lt;/p&gt;

&lt;p&gt;The following graph was made in Draw.io, and illustrates a probabilistic system.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;../assets/css/img/QuantumSystem.png&quot; alt=&quot;Quantum System&quot; style=&quot;width: 50%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;In this case, our system evolves as $\psi(t+dt)=U\psi(t)$ where $U$ equals:&lt;/p&gt;

\[\begin{bmatrix}
0.25 &amp;amp; 0 &amp;amp; 0.5 &amp;amp; 0.25 \\ 
0.75 &amp;amp; 0.25 &amp;amp; 0 &amp;amp; 0 \\ 
0 &amp;amp; 0.25 &amp;amp; 0 &amp;amp; 0.75 \\ 
0 &amp;amp; 0.5 &amp;amp; 0.5 &amp;amp; 0
\end{bmatrix}\]

&lt;p&gt;$U$ is called a doubly stochastic matrix because its sums and rows sum to $1$. It is, however, not unitary.&lt;/p&gt;

&lt;p&gt;To illustrate what is happening, if we start in the state $\psi=\left [0.1, 0.7, 0, 0.2] \right ]^{T}$ at time $t$, then-by the power vested in me by the method of matrix multiplication-at time $t+dt$ $\psi$ will equal $\left [0.075, 0.25, 0.325, 0.35 \right ]^{T}$. Reading out only the first entry, this is saying that at time $t$, the marble had a $10$ percent chance of being found at vertex $0$. At time $t+dt$ the marble has a $7.5$ percent chance of being found at vertex $0$.&lt;/p&gt;

&lt;p&gt;(To be continued)&lt;/p&gt;</content><author><name>Lawrence Borst</name></author><category term="quantum" /><category term="computation" /><category term="computing" /><category term="qubits" /><category term="introduction" /><summary type="html">During the past few months, I’ve been reading on quantum computation. For simplicity, I’ve listed all the resources in the bibliography for the ease of the reader. I strongly recommend to others who wish to learn about quantum computing to start with Quantum Computing for Computer Scientists and just skim through it. This is a slightly dumbed-down approach to quantum computing, but very practical nonetheless. Going through it will equip you with all the knowledge you’d need to make your own quantum computinng emulator, assuming you know a little programming. The next best thing is the quantum bible Quantum Computation and Quantum Information, which is more challenging but gives you a more grounded understanding of quantum computation, as well as some of the physics behind it. Besides those resources, I recommend anyone interest in quantum computation to play with Qiskit—a quantum emulator—and to read through its documentation. The stack exchange for quantum computation is also a useful resource. In this introductory article, I hope to lay out the groundworks for any beginner in the manner that these sources describe. I hope this will pique the interest of someone who may have heard of quantum computing, but has no clue what it really entails.</summary></entry><entry><title type="html">Investigating_consonance in microtonal music part 1</title><link href="http://localhost:4000/microtonal" rel="alternate" type="text/html" title="Investigating_consonance in microtonal music part 1" /><published>2020-11-20T00:00:00+01:00</published><updated>2020-11-20T00:00:00+01:00</updated><id>http://localhost:4000/Investigating_Consonance%20in%20Microtonal%20Music%20Part%201</id><content type="html" xml:base="http://localhost:4000/microtonal">&lt;p&gt;I see microtonal music as an often dissonant art. This perception owes itself in part to its strangeness, but also to scientific principles that make many types of microtonal music unattractive to the uninitiated. For example, imagine a pitch. Play it in your head, and then layer it with a second pitch just slightly above the first. What you’re hearing is a dissonant interval, and we shall go over the reason why that may be. We will attempt to categorize chords by their “pleasantness” to the ear, and show how this problem can be handled mathematically.&lt;/p&gt;

&lt;p&gt;But before we can dive into microtonality we need to understand Western tuning, how sound is perceived, and what makes a sound pleasant.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2&gt;The Octave, Pitches and Sound Waves&lt;/h2&gt;

&lt;p&gt;A good starting point is the octave. The octave is an interval, meaning it denotes a pitch relationship between two notes. The octave is a natural starting point when it comes to tuning. When I play a C4 and a C5 simultaneously on a piano, it will sound harmonious; “perfect”. In fact, the C4 and C5 “feel” similar, and musicians may use terms like “perfect octave” to describe such assonant intervals. You could say C5 sounds like a “higher” C4. But when we shift somewhere arbitrary, like E5, we would not say it sounds like a “higher” C4… But we could say it sounds like a higher E4-if you’re human, that is.&lt;/p&gt;

&lt;p&gt;A pitch in music is a perceptual property of sound. A sound is a pressure wave, and for a pitched sound, the pressure it carries to your ear will be periodic. A sound wave can be quite arbitrary; pitch will be perceived as long as it is periodic. Your brain is able to distinguish the frequency of sound, and from this frequency perceives a pitch. While we’ll talk a lot about “pitch”, I remind the reader that pitch is only a perception, and the frequency of sound waves is fundamentally where music happens physically.&lt;/p&gt;

&lt;p&gt;So what makes the octave special? It is this: if the “lower” sound has pitch $x$, then the “higher” sound will have pitch $2x$. Precisely $2x$. If we begin to deviate, by playing at say a frequency of $2x+\epsilon$, the resulting interval will sound less harmonious.&lt;/p&gt;

&lt;p&gt;I don’t think we fundamentally know &lt;b&gt;why&lt;/b&gt; our brains favor intervals such as this—in fact, it isn’t even clear if animals have opinions about intervals at all—but it seems to be the way it works. We can, however, make a guess why the two pitches sound “similar”. Maybe it is because a sound with frequency $2x$ necessarily also has frequency $x$. This does, however, not reveal why the magic number is $2$, instead of $3$ or something. (But the brain &lt;b&gt;does&lt;/b&gt; favor sounds at frequencies $x$ and $3x$ played simultaneously. This would be a perfect twelfth, and sounds harmonious, but nothing like duplicating a sound).&lt;/p&gt;

&lt;h2&gt;Equal Temperament Tuning&lt;/h2&gt;

&lt;p&gt;We shall briefly look at the Western tuning system, because microtonality naturally extends it. Western tuning—in particular what we might call equal temperament—can be constructed from two pieces of information: a reference pitch, and the number of equal intervals we split an octave into. The reference pitch acts like an “origin”, and in Western music we could set it at A4, which we define to be $440$Hz (ocillations/repetitions per second of a sound).&lt;/p&gt;

&lt;p&gt;So what should A5 be, one octave up? Well, $880$Hz, from the previous discussion. A6? Perhaps it is tempting to bet on $1320=3(440)$, but it is 1760Hz, because the relationship between A6 and A5 is the same between A5 and A4. That is, A6 doubles the frequency associated with A5, the same way A5 doubles that with A4.&lt;/p&gt;

&lt;p&gt;The reader may recognize that when we choose to split the octave into equal intervals, we are forced not to cut the octave into equal frequencies, but rather to pick frequencies in geometric progression. This multiplicative behavior may seem striking, but in a way pitch perception is not much different from how humans perceive many other stimuli (see Steven’s power law). Loudness, touch sensitivity, pitch and warmth perception all follow power laws with respect to our “perceived intensity”. In a sense, pitch and vibrations are related by Steven’s power law. Mathematically it doesn’t really matter if it’s a power law or a linear law… But I digress.&lt;/p&gt;

&lt;p&gt;So if we start with A4 at $440$Hz, what frequency should the next note (Bb4) be? Well, say we wish to split A4 to A5 into twelve equal intervals, and each interval takes us further by a (multiplicative) factor of x. Then, moving up 12 steps (semitones), we get $440x^{12}=880$, giving $x=\sqrt[12]{2}$. Then Bb4 is at $440\sqrt[12]{2}\approx 466.16$.&lt;/p&gt;

&lt;p&gt;At this point we can construct all $88$ keys on the keyboard with their associated frequencies. We’ll leave tuning at that for now, though the subject goes much deeper in practice—there are infinite families of tuning systems to draw from! The tuning system here is most commonly used on synthesizers, electric guitars, and keyboard instruments, just to name a few.&lt;/p&gt;

&lt;h2&gt;The Mathematics of Consonant Intervals&lt;/h2&gt;

&lt;p&gt;We’re nearing the prerequisite knowledge to understand how to tackle microtonality. But to solve the problem of finding pleasant chords in it, we need to understand “consonance” and “dissonance”.&lt;/p&gt;

&lt;p&gt;Where “consonance” conveys something pleasant, “dissonance” conveys something unnerving. This is the same to how “consonance” and “dissonance” are used to talk about intervals. The mathematical machinery of consonant intervals is every bit as harmonious as its sound.&lt;/p&gt;

&lt;p&gt;Roughly, when two notes $A$ and $B$ are heard, the “closer” the ratio between their associated frequencies $f(A):f(B)$ is to a rational number of the form $a:b$ with small $a$ and $b$ (coprime), the more harmonious it sounds.&lt;/p&gt;

&lt;p&gt;For example, $f(A5):f(A4)=2:1$ is very harmonious. This is an the pitch ratio associated with an octave. For a perfect fifth, which is considered the “second-most” consonant interval, we have something like $f(A5):f(E5)=\sqrt[12]{2^{7}}\approx 1.498$ because a perfect fifth has 7 “steps” (semitones). This number is irrational, and so you may expect it to be very dissonant. But it is also awfully close to a “nice rational number”, namely $3:2=1.5$ That is why it sounds “nice”. In fact, we can see that octaves and perfect fifths sound “nice” everywhere on the keyboard, as long as the ratio of frequencies is conserved. We humans are so accustomed to thinking about pitch in this relative way that we are better at distinguishing the interval two notes make than any one note’s exact position! This is called relative pitch. I’ve heard of people with absolute pitch—the perception of pitches as “absolute” frequencies—feeling dumbfounded that most people perceive music in such a complex, roundabout way.&lt;/p&gt;

&lt;p&gt;But how do we characterize “closeness?” I have searched online for this, but couldn’t find any resources. On the internet there are scores of lists with these ratios related to the $12$ intervals we all know and love. But no derivation. I have chosen to show a personal method based on continued fractions. Every number $x\in \mathbb{R}$ can be expressed in the form:&lt;/p&gt;

\[x=a_{0}+\frac{1}{a_{1}+\frac{1}{a_{2}+\frac{1}{\ddots}}}\]

&lt;p&gt;Where $a_{i}\in \mathbb{Q}$. The above expression is the continued fraction expansion of $x$, and we often write $x=\left [ a_{0};a_{1}, a_{2}, a_{3},\cdots  \right ]$. For rational $x$, this expression terminates. Conversely, every terminating continued fraction is rational. For irrational $x$, it doesn’t terminate. We can draw a lot of similarities between continued fraction and decimal expansions… But continued fractions are more powerful. The continued fraction of $x$ is the best approximation to $x$ in the following sense: if $p/q$ (coprime) is any one of those fractions obtained by truncating the full continued fraction, then it is impossible to find $s &amp;lt; q$ in $\mathbb{Z}$ such that $r/s$ is closer to $x$. Furthermore, the error is less than $\frac{1}{q^{2}}$.&lt;/p&gt;

&lt;p&gt;We therefore choose to define the pitch ratio “close” to $x=\left [ a_{0};a_{1}, a_{2}, a_{3},\cdots  \right ]$ as the truncated $\left [ a_{0};a_{1}, a_{2}  \right ]=a_{0}+1/(a_{1}+1/a_{2})$. We could have more terms, of course, but I don’t think this is necessary.&lt;/p&gt;

&lt;p&gt;So let’s derive the coefficients. I’ll outline the derivation from The Princeton Companion of Mathematics; the trick comes from 13th century Indian and Arabian mathematics, and is based on the Euclidean algorithm. If we pick $m$ and $n$, and find unique quotient and remainder $q_{1}$, $r_{1}$ with $m=q_{1}n+r_{1}$ (constrained in the usual way), then $m/n=q_{1}+r_{1}/n=q_{1}+1/F$, where $F=n/r_{1}$. Using the Euclidean algorithm again, we can write $n=q_{2}r_{1}+r_{2}$, giving $F=q_{2}+r_{2}/r_{1}$. The next step is to produce an expression for $r_{2}/r_{1}$. Doing so indefinitely will produce the infinite fraction expansion in the following form:&lt;/p&gt;

\[x=q_{0}+\frac{1}{q_{1}+\frac{1}{q_{2}+\frac{1}{\ddots}}}\]

&lt;p&gt;Let’s apply this to the perfect fifth. The pitch ratio associated with a perfect fifth is $x=\sqrt[12]{2^{7}}$. The integer part of $x$ is $1$, by inspection. The remainder is roughly $0.4983$, and the reciprocal of that ($F$ in the above) is $2.0068$ with an integer part of $2$. Stopping here gives $3/2$, which is what we want. But we will not stop there, mainly because a diminished sixth is also $3/2$ if we choose to stop here; the diminished sixth should be very dissonant. Instead, going one step furher, we have $F=147$! A large jump. We then find that $x$ is “close” to $442/295$! That sounds terrible.&lt;/p&gt;

&lt;p&gt;Musicians might cringe at this method, because the fraction is so ridiculously complicated where they might expect it to be simpler (because a perfect fifth his consonant). But here we exactly want large fractions. Consider the following expressions:&lt;/p&gt;

\[x=1+\frac{1}{2}\]

\[y=1+\frac{1}{2+\frac{1}{147}}\]

&lt;p&gt;Looking at the expressions, we can tell that they are really close because $1/147$ is close to zero. Furthermore, $x$ is our first “best” approximation to the perfect fifth, and $y$ is our second “best”… And there’s nothing in between that’s better. But that’s saying that a perfect fifth is already very close to $3/2$. This means that the perfect fifth is very close to a “nice” rational number. In fact, we’re kind of measuring the “irrationality” of $\sqrt[12]{2^{7}}$ based on the way expression unfolds. This has been done before, but not in a way that can be usefully applied to music.&lt;/p&gt;

&lt;p&gt;We will now have to define precisely how we’re going to measure how dissonant the intervals are based on these expressions. There’s no one way to do this, but it’s clear that we can take the octave as the most consonant interval by default, and by the irrationality of all the other intervals we can assume $x$ has an infinite expansion, and so we can find the first $n$ terms. Some exhausting hours of work showed me that no clear pattern emerges by just looking at the first $5$ or so terms. For instance, the continued fraction associated with the tritone (besides a minor second, the “ugliest” interval) is $[1; 2, 2, 2, 2…]$. We expect this to be dissonant because all the numbers are rather small. But looking at the minor 6th (quite a “nice” interval) we get $[1, 1, 1, 2, 2…]$. But shouldn’t that be even more dissonant?! All this may be a fool’s errand. Yet I couldn’t find any papers; even the so-called “limit theory” in music (which deals with questionsl like ours) I couldn’t find proper justification for.&lt;/p&gt;

&lt;p&gt;But looking at a &lt;a style=&quot;color:blue;&quot; href=&quot;http://cc.oulu.fi/~tma/TAPANI28.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferer&quot;&gt;paper&lt;/a&gt; connecting infinite radicals with a measure of the “irrationality” of a number, I was led to believe that the most important thing is the magnitude of the first fractional term (kept small for consonance), and that of the largest term we’ll encounter after (large for consonance). Indeed, going a bit further, we obtain $[1, 1, 1, 2, 2, 1, 3…]$. I’ve never been this happy to see a $3$!&lt;/p&gt;

&lt;p&gt;Because the tritone corresponds to a pitch ratio of $2^{1/2}$, the “twos” in the expansion never terminate. But looking at the minor sixth, we get a $3$, which is more consonant than $2$… And $12$ terms in—after a long streak of $1$’s, $2$’s and some $3$’s, we get a whopping $30$.&lt;/p&gt;

&lt;p&gt;Now, some of these continued fractions converge slooowly. The “Just Noticeable Difference” between pitches is quite precise (it also follows a power law of sorts, namely Weber’s law), so I wanted to investigate the relationship between the different truncations of the continued fractions &lt;b&gt;up to the point&lt;/b&gt; where we can no longer distinguish frequency differences. I also wanted to investigate whether our perception of consonance is related to a point where our brains will decide that the sound can be reasonably approximated by a simple ratio. This would relate to the truncation associated with the first big “jump” in our continued fraction, rather than a set number of iterations. Such a “jump” can definitely be characterized in absolute terms, and this can all be happily investigated. And that is the objective for part II.&lt;/p&gt;

&lt;p&gt;(To be continued)&lt;/p&gt;

&lt;h1&gt;References&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Gowers, Timothy, June Barrow-Green, and Imre Leader. 2008. &lt;i&gt;The Princeton Companion to Mathematics&lt;/i&gt;. Princeton, N.J.: Princeton University Press.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Hancl, J., Matala-aho, T., and Pulcerova, S., 2004. “Continued Fractional Measure Of Irrationality.” [ebook] Available at: &lt;a href=&quot;http://cc.oulu.fi/~tma/TAPANI28.pdf&quot;&gt;http://cc.oulu.fi/~tma/TAPANI28.pdf&lt;/a&gt; [Accessed 18 November 2020].&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Shapira Lots, Inbal, and Lewi Stone. “Perception of Musical Consonance and Dissonance: An Outcome of Neural Synchronization.” Journal of the Royal Society Interface, vol. 5, no. 29, Dec. 2008, pp. 1429–34. PubMed Central, doi:10.1098/rsif.2008.0143.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Lawrence Borst</name></author><category term="music" /><category term="microtonal music" /><category term="chords" /><summary type="html">I see microtonal music as an often dissonant art. This perception owes itself in part to its strangeness, but also to scientific principles that make many types of microtonal music unattractive to the uninitiated. For example, imagine a pitch. Play it in your head, and then layer it with a second pitch just slightly above the first. What you’re hearing is a dissonant interval, and we shall go over the reason why that may be. We will attempt to categorize chords by their “pleasantness” to the ear, and show how this problem can be handled mathematically. But before we can dive into microtonality we need to understand Western tuning, how sound is perceived, and what makes a sound pleasant.</summary></entry><entry><title type="html">Lagrange interpolation</title><link href="http://localhost:4000/lagrange-interpolation" rel="alternate" type="text/html" title="Lagrange interpolation" /><published>2020-11-15T00:00:00+01:00</published><updated>2020-11-15T00:00:00+01:00</updated><id>http://localhost:4000/Lagrange%20Interpolation</id><content type="html" xml:base="http://localhost:4000/lagrange-interpolation">&lt;p&gt;To launch this website, let’s begin with the simple but elegant idea of Lagrange interpolation.&lt;/p&gt;

&lt;p&gt;The aim is to find a polynomial of degree $n$ or less that passes through $n-1$ points. People educated in linear algebra often can see that this is possible just by setting up the associated system of linear equations. For completeness:&lt;/p&gt;

&lt;p&gt;If $P(x)=a_{0}+a_{1}x+\cdots+a_{n}x^{n}$ and $P(x_{0})=y_{0}$, $P(x_{1})=y_{1}$,$\cdots$,$P(x_{n-1})=y_{n-1}$ then we have the following system:&lt;/p&gt;

&lt;!--more--&gt;

\[y_{0}=a_{0}+a_{1}x_{0}+\cdots+a_{n}x_{0}^{n}\]

\[y_{1}=a_{0}+a_{1}x_{1}+\cdots+a_{n}x_{1}^{n}\]

\[\vdots\]

\[y_{n-1}=a_{0}+a_{1}x_{n-1}+\cdots+a_{n}x_{n-1}^{n}\]

&lt;p&gt;$n$ unknowns and $n$ equations. This we know is solveable. You can solve this by hand for a few points—and a computer will have no trouble with thousands of points—but our aim is to find a closed-form formula for the interpolating polynomial. Looking at this set of equations might not get us anywhere close to solving this problem.&lt;/p&gt;

&lt;p&gt;Instead of revealing the trick, I encourage the reader to think about the problem. A good line of reasoning is something like this: “The main difficulty is that when you assume a function $P(x)$ and tweak the coefficients to fit through some point, the polynomial will inevitably slip away from other points.” It is a bit like trying to get rid of a crease on your bedsheets; flattening one part will cause a pinch to pop up elsewhere. So then we might ask ourselves: “Is there a way to treat an individual point without disturbing the rest?”&lt;/p&gt;

&lt;p&gt;Well, consider one of the points we’re trying to pass through. If we can associate a polynomial (which is what the question is about) with an individual point that’s “nice” in a way that “does not disturb the others”, we would need a total of $n$ polynomials to address all points. Every polynomial should act in a similar way to its associated point, because no point is “special”. So something should be quite “natural” about this polynomial. It should pass through a point, and as for the others… Well, be equal to $0$ at the others (can you think of anything else?). This is quite a natural thing to do with the other points, and as it turns out, it is easy to find the associated polynomial.&lt;/p&gt;

&lt;p&gt;Let’s say $P_{i}(x)=0$ at $x=x_{j}$ for all $j \neq i$ and $P_{i}(x_{i})=y_{i}$. By the Fundamental Theorem of Algebra, we have:&lt;/p&gt;

\[P_{i}(x)=C_{i}\prod_{j \neq i} (x - x_{j})\]

&lt;p&gt;For some constant $C_{i}$. Plugging in $P_{i}(x)=y_{i}$ gives $C_{i}=y_{i} / \prod_{j \neq i} (x_{i} - x_{j})$.&lt;/p&gt;

&lt;p&gt;We have tamed the problem in the sense that we can find a polynomial of degree $n-1$ through a given point, that equals $0$ at all the other points. The attempt so far may not lead anywhere—as happens often when you’re tackling a problem for the first time—but some thought reveals that the sum of all these polynomials goes through the points $(y_{0}, x_{0}), (y_{1}, x_{1}),\cdots,(y_{n-1}, x_{n-1})$. Therefore&lt;/p&gt;

\[P(x)=\sum_{i=0}^{n-1}y_{i} \frac{\prod_{j \neq i} (x - x_{j})}{\prod_{j \neq i} (x_{i} - x_{j})}\]

&lt;p&gt;Solves the problem.&lt;/p&gt;</content><author><name>Lawrence Borst</name></author><category term="lagrange polynomials" /><category term="interpolation" /><category term="approximation" /><category term="mathematics" /><category term="math" /><summary type="html">To launch this website, let’s begin with the simple but elegant idea of Lagrange interpolation. The aim is to find a polynomial of degree $n$ or less that passes through $n-1$ points. People educated in linear algebra often can see that this is possible just by setting up the associated system of linear equations. For completeness: If $P(x)=a_{0}+a_{1}x+\cdots+a_{n}x^{n}$ and $P(x_{0})=y_{0}$, $P(x_{1})=y_{1}$,$\cdots$,$P(x_{n-1})=y_{n-1}$ then we have the following system:</summary></entry></feed>